[
  {
    "objectID": "code_1_intro.html",
    "href": "code_1_intro.html",
    "title": "Week #1",
    "section": "",
    "text": "This contains notes about the introductory meeting, intended to:\n\nReview on the use of GitHub for collaborative projects.\nExplore the new features offered by the new development of RStudio: Quarto.\n\n\n\nGitHub is an internet hosting service designed for software development and version control using Git. Basically, GitHub facilitates to work in a collaborative manner through multiple features designed to efficiently control changes made by users during a working session.\nUseful desktop applications:\n\nGitHub Desktop\nGitKraken\n\nKey glossary:\n\n\n\n\n\n\n\n\n#\nConcept\nDescription\n\n\n\n\n1\nversion control\nis the practice of tracking and managing changes to software code\n\n\n2\nrepository\nis a code hosting platform for version control and collaboration.\n\n\n3\nbranch\nis a unique set of code changes with a unique name.\n\n\n5\nclone\nis to copy a remote repository into your local\n\n\n4\nremote/local\nthey are Git repositories hosting code at different locations. ‚ÄúRemote‚Äù means located on the internet or another network. ‚ÄúLocal‚Äù means located in your machine.\n\n\n5\norigin\nis the remote version of a given repository.\n\n\n6\nmain/master\nis the name of the default branch. In the past ‚Äúmaster‚Äù was the default. Since 2020, ‚Äúmain‚Äù is the default name of all new source code.\n\n\n7\ncommit\nis a ‚Äúrevision‚Äù or individual set of changes to a file or group of files within a branch.\n\n\n8\npull / fetch\nis the action to fetch and download content from a remote repository to update your local repository.\n\n\n9\npush\nis the action to upload content from a local repository to update its remote version (all within a given branch).\n\n\n10\npull request\nis to let the repository owner to know about changes you‚Äôve made to his code pushing to a new named branch. It allows to discuss, review, accept/reject the changes, and eventually accept and merge your branch to update the old version (normally, the ‚Äúmain‚Äù branch).\nNote: This action allows users to appear into the ‚Äúcontributors‚Äù list of a repository.\n\n\n11\nmerge\nis the action of combining two branches into one, and putting a combined ‚Äúforked history‚Äù.\n\n\n12\nrebase\nis moving or combining a sequence of commits to a new base commit.\n\n\n13\nfast-forward\nis merging when there is a direct linear path from the source branch (updating from) to the target branch (the one updating to).\n\n\n14\nstash*\nit temporarily shelves changes you‚Äôve made to your copy so you can work on something else, and then come back and re-apply them later. *very useful when you need to pull and update from the remote but you have been working on your local file.\n\n\n15\nrevert\nis the ‚Äúundo‚Äù action to safely undoing changes. It creates a new commit that inverses changes.\n\n\n16\nfork\nis to copy others‚Äô repository into your account. Your forked repo will act as an independent branch, so you can commit changes and put pull request from the forked repo.\n\n\n17\nIssues\nis a section of any GitHub repository that allows to track issues experienced by users. Particularly useful for packages. Issues can be quoted and linked to pull requests.\n\n\n18\nPATs\nstands for Personal Access Tokens, which are an alternative authentication method to passwords and SSH\n\n\n19\nlfs\nthe lfs stands for Large File Storage. It allows you to track heavy files that are not allowed by default.\n\n\n20\nGitHub Actions\nis the way to automate continuous integration and delivery (CI/CD). Particularly, CI is a very important a software practice for package development, since it allows to detect errors on different operative systems via multiple automated tests.\n\n\n\n\n\n\nQuarto is an open-source scientific and technical publishing system built on Pandoc. It allows to combine R, Python, Julia, and Java (Observable JS) for the design of reports, applications, websites, blogs, scientific publications, and more...\nKey concepts to review:\n\nqmd\nrender\nyml\ncss\n\nLearn more: https://quarto.org\n\n\n\nDownload Quarto or the latest version of RStudio\n\n\n\n\nToday, we will work together on the creation of our own hex-sticker using both Quarto and GitHub.\n\n\n\nClone a repository:\n\nOpen your web-browser and go to https://github.com/adriancorrendo/Stats_Hub_2022\nFork to your own GitHub account.\nGo to your forked repo and click on the ‚ÄúCode‚Äù button and copy the https address\n\n\nOpen Rstudio and go to File > New Project‚Ä¶ > version control > Git, and paste the link.\n\nGo to the logo > code and open the hexsticker_stats_hub.qmd file.\nInstall the required packages.\nModify the logos or create an alternative using the *.qmd file.\nCreate a commit. with your changes.\nPush (send changes).\nContribute with a Pull Request.\nWait for the PR to be merged into the original repo."
  },
  {
    "objectID": "code_2_bayes_1.html",
    "href": "code_2_bayes_1.html",
    "title": "Week #2",
    "section": "",
    "text": "This article is intended to provide a brief introduction to key concepts about Bayesian theory and differences with the traditional Frequentist approach:\n\n\n\n\n\n\nImportant\n\n\n\nNeither of both, Frequentist or Bayesian, are always the most suitable solution for your analysis. However‚Ä¶.üòâ\n\n\nFor this reason, today we are going to discuss and compare both approaches.\nLet‚Äôs watch some short videos about it\n\n\n\n\n\n\nWhat do you think?\n\nOpen discussion‚Ä¶.\n\n\n\n\n\nPerhaps, the main disagreement between Frequentism and Bayesianism is about the TRUTH.\nThe Frequentism vision is heavily rooted on the actual existence of the TRUTH. Every time we estimate a model‚Äôs parameter, we expect to approximate to a true value. It is named ‚Äúfrequentism‚Äù because it is based on the frequency of repeated events.\nFor example, if we want to assess the probability of getting a #6 when rolling a dice, Frequentism says that ‚ÄúIf we roll a dice close to infinite times, the proportion of #6 out of the total number of rolls will approach 16.7% (the theoretical probability)‚Äù. Thus, Frequentism makes inference conditional to an ideal, ‚Äútheoretical‚Äù condition of repeating the same experiment infinite times. In other words, conclusions rely on events we have not observed.\nThe Bayesian approach instead, DOES NOT assume the existence of the TRUTH. In contrast, it is based on PROBABILITIES & BELIEFS.\nPROBABILITIES: For the Bayesian vision, everything is a matter of probability. Any fact or result about an ‚Äúestimate‚Äù could range from extremely unlikely to extremely likely. However, anything is considered completely true or false.\nBELIEFS: here is probably the most important point of the Bayesian vision. Bayesian models allow to introduce (and update) prior knowledge on a topic, introducing our own certainty or uncertainty about events. EVEN IF WE DON‚ÄôT KNOW ANYTHING about it (spoiler alter: uninformative prior!).\nBayesianism considers probability as an expression of the degree of certainty (or uncertainty).\nFollowing the same example with the dice roll, Bayesian interpretation says that ‚Äúwe are, a priori, 16.7% certain we are going to get a #6‚Äù. The, Bayesianism makes inference conditional to the data we observed. We basically test the likelihood of a prior hypothesis being true given the observed data, and we generate a new ‚Äúlikelihood‚Äù of updated hypothesis being true given the observed data.\nAnd now, to compare previous beliefs (prior) to updated knowledge (posterior) we can introduce the concept of Bayes Factor, which is a ratio between to candidate statistical models represented by marginal likelihood to measure the support for one model over the other. For example, if we have 0.167 as a prior belief of obtaining a #6, and our updated likelihood (after combining with observed data) results 0.334:\n\\[\nBayes Factor = \\frac{0.334}{0.167} = 2\n\\]\nThus, our updated hypothesis is twice as likely to be true than our prior hypothesis given the observed data.\nTherefore, when we analyze our data:\n\nFrequentism assumes models being fixed and our data random (maximum likelihood, conditional to theoretical events).\nBayesian assumes that models can vary around our data (conditional to observed data)\n\n\n\n\n\n\n\nTip\n\n\n\nFor simple models, however, the two approaches would be practically indistinguishable‚Ä¶\nHINT: think about uninformative prior knowledge!\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe structure of the Bayesian theory is similar to the Human Logic process:\n(i) we have some data,\n(ii) we have beliefs about the underlying process,\n(iii) combining both, we can update our beliefs.\n\n\n\n\n\n\\[ P(A_{true} | B)  = \\frac{P(B|A_{true}) * P(A_{true})}{P(B)}\\] \\[ Posterior  = \\frac{Likelihood * Prior}{Evidence}\\]\n\n\n\n\n\n\n\n\n\n\nPriors are basically a formalization of our believes in a form of a mathematical function describing a ‚Äúdistribution‚Äù. First, it depends on the nature of the variable of interest, which could be ‚Äúdiscrete‚Äù or ‚Äúcontinuous‚Äù. Second, it depends on what we know (or not) about the process of interest‚Ä¶\n\n\n\nThere is a very important difference between Frequentism and Bayesianism in terms of error interpretation. Let‚Äôs say we estimate 95% confidence and credible intervals for \\(\\theta\\):\n\nConfidence intervals (Frequentist): ‚ÄúIf we repeat the experiment infinite times, 95% of the estimated confidence intervals will contain the true value of \\(\\theta\\) (based on repeated measurements)‚Äú. Note that since \\(\\theta\\) is fixed, it can only be within or outside the interval.\nCredible intervals (Bayesian): In contrast, Bayesianism has a ‚Äúliteral‚Äù interpretation of the error saying, *‚Äúthere is a probability of 95% that the parameter* \\(\\theta\\) lies within this credible interval‚Äù. This is a range of probable values. Note:‚ÄùGiven that the prior is correct‚Äù‚Ä¶\n\n\n\n\n\n\n\nBayesian Models: A Statistical Primer for Ecologists. Hobbs and Hooten\nBringing Bayesian Models to Life. Hooten and Hefley\nNot from biological sciences but still very good:\nStatistical Rethinking. McElreath\nOn Bayesian workflow/philosophy:\nBayesian workflow\nScientific Reasoning: The Bayesian Approach. Howson and Urbach\n\n\n\nBayesian Data Analysis 3\n\n\n\nMakowski et al., 2020\n\n\n\nBlog: Statistical Modeling, Causal Inference, and Social Science. Gelman et al.\nPodcast: Learning Bayesian Statistics"
  },
  {
    "objectID": "code_3_bayes_2.html",
    "href": "code_3_bayes_2.html",
    "title": "Week #3",
    "section": "",
    "text": "This is a follow-up article from Bayes#1. Still, we do have numerous important concepts in order to understand what the computational codes are doing behind scenes when running a Bayesian analysis.\n\n\n\n\n\n\nNote\n\n\n\nToday‚Äôs Topics:\nComputing posterior distributions:\n#1. Acceptance/Rejection Sampling Basics:\n#2. Markov Chain Monte Carlo (MCMC) -more efficient than AR sampling-.\nPackages for Bayesian analysis in R:\n#3. brms\n#4. rstan\n#5. rjags\n\n\n\n\n\nlibrary(latex2exp)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(brms)\nlibrary(tidybayes)\n\n\n\n\n\n\n1. Generate proposal parameter values\n2. Generate data with those parameters\n3. Compare the simulated data with the observed data = ‚Äúdifference‚Äù\n4. ‚ÄúAccept‚Äù that combination of parameters if the difference < predefined acceptable error. ‚ÄúReject‚Äù if the difference > predefined acceptable error.\nSee an example:\nUsing data of yield vs plant density in corn:\n\n\n\n\n\n\\[ y = \\beta_0 + x \\cdot \\beta_1 - x^2 \\cdot \\beta_2\\]\n\nGenerate proposal parameter values using the prior ditributions:\n\n\\[\\beta_0 \\sim uniform(4, 6)\\]\n\\[\\beta_1 \\sim uniform(1, 3)\\]\n\\[\\beta_2 \\sim uniform(0.5, 2)\\]\n\\[\\sigma \\sim Gamma(2, 2)\\]\n\nset.seed(567)\nb0_try <- runif(1, 4, 6)  # Parameter model\nb1_try <- runif(1, 1, 3)  # Parameter model \nb2_try <- rgamma(1, .5, 2) # Mathematical equation for process model\nmu_try <- b0_try + x*b1_try - (x^2)*b2_try\nsigma_try <- rgamma(1, 2, 2)\n\n\nGenerate data with those parameters\n\n\n\nset.seed(567)\ny_try <- rnorm(n, mu_try, sigma_try)  # Process model\n\n\nCompare the simulated data with the observed data = ‚Äúdifference‚Äù\n\n\n# Record difference between draw of y from prior predictive distribution and\n# observed data\ndiff[k, ] <- sum(abs(y - y_try))\n\n\n‚ÄúAccept‚Äù (gold) that combination of parameters if the difference < predifined acceptable error. ‚ÄúReject‚Äù (red) if the difference > predifined acceptable error.\n\n\nplot(x, y, xlab = \"Plant density\", \n     ylab = \"Observed yield\", xlim = c(2, 13), ylim = c(5, 20),\n     typ = \"b\", cex = 0.8, pch = 20, col = rgb(0.7, 0.7, 0.7, 0.9))\npoints(x, y_hat[k,], typ = \"b\", lwd = 2, \n       col = ifelse(diff[1] < error, \"gold\", \"tomato\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, what if whe change the priors:\n\n\n\n\n\nNow, do many tries\n\nfor (k in 1:K_tries) {\n    \n    b0_try <- runif(1, 2, 10)  # Parameter model\n    b1_try <- rnorm(1, 2.2, .5)  # Parameter model \n    b2_try <- rgamma(1, .25, 2) # Mathematical equation for process model\n    mu_try <- b0_try + x*b1_try - (x^2)*b2_try\n    sigma_try <- rgamma(1, 2, 2)\n\n    y_try <- rnorm(n, mu_try, sigma_try)  # Process model\n    \n    # Record difference between draw of y from prior predictive distribution and\n    # observed data\n    diff[k, ] <- sum(abs(y - y_try))\n    \n    # Save unkown random variables and parameters\n    y_hat[k, ] <- y_try\n    \n    posterior_samp_parameters[k, ] <- c(b0_try, b1_try, b2_try, sigma_try)\n}\n\nAcceptance rate\n\nlength(which(diff < error))/K_tries\n\n[1] 0.031291\n\n\nPriors versus posteriors:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhist(y_hat[which(diff < error), 25], col = \"grey\", freq = FALSE)\nabline(v = y[25], col = 'gold', lty = \"dashed\", lwd = 5)\n\n\n\n\n\n\n\n\n\nLet‚Äôs get started\n\n\n\n\n\nMarkov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics (Brooks et al., 2011).\nIn a nutshell, MCMC represents a family of algorithms that facilitate the generation of random samples from probability distributions that are difficult (e.g.¬†high-dimensional) to sample directly. They are ‚Äúchains‚Äù because the random samples are produced in consecutive-dependent steps (i.e.¬†step 2 comes from step 1, step 3 comes from step 2, ‚Ä¶.). This details is a game-changer to more efficiently use and integrate Monte Carlos simulations.\nSources on MCMC:\nhttps://www.mcmchandbook.net/\nhttps://cran.r-project.org/package=MCMCpack\nhttps://cran.r-project.org/package=mcmc\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=3759243\n\n\n\n\n\n\n\n\nDocumentation: https://paul-buerkner.github.io/brms/\nBug-reports: https://github.com/paul-buerkner/brms/issues\nbrms is a very handy R-package that facilitates running Bayesian models using a relatively simple syntax. It is basically and interface that runs ‚ÄúStan‚Äù behind the scenes. It uses a syntax quite similar to the lme4 package.\nIt allows to use several different type of distributions and link functions for models that are linear, counts, survival, response, ordinal, zero-inflated, etc.\nDue to its relatively simple syntax, today, we are going to start our Bayesian coding with brms.\nMore about brms at https://www.jstatsoft.org/article/view/v080i01\n\n\n\nLet‚Äôs fit the example using the brms package.\n\n\n\n\n# Set up pars\nWU = 1000\nIT = 5000\nTH = 5\nCH = 4\nAD = 0.99\n\n\n\n\n\n#| eval: false\n#| echo: true\n\n# 01. Run models\n\nbayes_model <- \n\n  brms::brm(\n  #Priors\n  prior = c(\n    #B0, Intercept\n    prior(prior = 'normal(8, 8)', nlpar = 'B0', lb = 0),\n    #B1, Linear Slope\n    prior(prior = 'normal(2, 4)', nlpar = 'B1', lb = 0),\n    #B2, Quadratic coeff\n    prior(prior = 'normal(0.001, 0.5)', nlpar = 'B2', lb = 0) ),\n    # Sigma  \n    #prior(prior = 'gamma(15,1.3)', class = \"sigma\") ),  \n    # Population prior (median and sd)\n    \n    # Formula\n  formula = bf(y ~  B0 + B1 * x - B2 * (x^2),\n               # Hypothesis\n               B0 + B1 + B2 ~ 1,\n               nl = TRUE), \n  # Data  \n  data = data_frame, sample_prior = \"yes\",\n  # Likelihood of the data\n  family = gaussian(link = 'identity'),\n  # brms controls\n  control = list(adapt_delta = AD),\n  warmup = WU, iter = IT, thin = TH,\n  chains = CH, cores = CH,\n  init_r = 0.1, seed = 1) \n\nCompiling Stan program...\n\n\nStart sampling\n\n# 02. Save object\nsaveRDS(object = bayes_model, file = \"bayes_model.RDS\")\n\nbayes_model <- readRDS(file = \"bayes_model.RDS\")\n\n# 03. Visual Diagnostic\nplot(bayes_model)\n\n\n\n# Visualize model results\nbayes_model\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ B0 + B1 * x - B2 * (x^2) \n         B0 ~ 1\n         B1 ~ 1\n         B2 ~ 1\n   Data: data_frame (Number of observations: 46) \n  Draws: 4 chains, each with iter = 5000; warmup = 1000; thin = 5;\n         total post-warmup draws = 3200\n\nPopulation-Level Effects: \n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nB0_Intercept     6.16      0.90     4.36     7.96 1.00     2462     2456\nB1_Intercept     1.93      0.26     1.42     2.45 1.00     2386     2619\nB2_Intercept     0.11      0.02     0.08     0.15 1.00     2395     2591\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.75      0.08     0.60     0.93 1.00     2969     2807\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# Compare vs traditional linear model (lm)\ndata_frame_q <- data_frame %>% mutate(x2 = x^2)\n\nlm(data = data_frame_q, formula = y ~ x + x2)\n\n\nCall:\nlm(formula = y ~ x + x2, data = data_frame_q)\n\nCoefficients:\n(Intercept)            x           x2  \n     6.1324       1.9401      -0.1127  \n\n\n\n\n\n\n# Create predictions\nm1 <- data_frame %>% \n  ungroup() %>% \n  dplyr::select(x) %>% \n  group_by(x) %>% filter(x == max(x)) %>% \n  ungroup() %>% unique() %>% rename(max = x) %>% \n  # Generate a sequence of x values\n  mutate(data = max %>% purrr::map(~data.frame(\n    x = seq(0,.,length.out = 400)))) %>% \n  unnest() %>% dplyr::select(-max) %>%\n  \n  #add_linpred_draws(m1, re_formula = NA, n = NULL) %>% ungroup()\n  # use \".linpred to summarize\"\n  tidybayes::add_predicted_draws(bayes_model, \n                                 re_formula = NA, ndraws = NULL) %>% ungroup()\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(data)`\n\n# Summarize\nm1_quantiles <- m1 %>% \n  group_by(x) %>% \n  summarise(q025 = quantile(.prediction,.025),\n            q010 = quantile(.prediction,.10),\n            q250 = quantile(.prediction,.25),\n            q500 = quantile(.prediction,.500),\n            q750 = quantile(.prediction,.75),\n            q900 = quantile(.prediction,.90),\n            q975 = quantile(.prediction,.975))\n\n# Plot\nm1_plot <- ggplot()+\n  # 95%\n  geom_ribbon(data = m1_quantiles, alpha=0.60, fill = \"cornsilk3\",\n              aes(x=x, ymin=q025, ymax=q975))+\n  # 80%\n  geom_ribbon(data = m1_quantiles, alpha=0.25, fill = \"cornsilk3\",\n              aes(x=x, ymin=q010, ymax=q900))+\n  # 50%\n  geom_ribbon(data = m1_quantiles, alpha=0.5, fill = \"#8a817c\",  \n              aes(x=x, ymin=q250, ymax=q750))+\n  geom_path(data = m1_quantiles,\n            aes(x=x, y=q500, color = \"brms()\"), size = 1)+\n  geom_point(data = data_frame, aes(x=x, y=y, color = \"brms()\"), alpha = 0.25)+\n  # Add LM curve\n  geom_smooth(data = data_frame, aes(x=x, y=y, color = \"lm()\"),  \n              method = \"lm\", formula = y ~ poly(x,2), se = T)+\n  scale_color_viridis_d()+\n  scale_x_continuous(limits = c(0,12), breaks = seq(0,12, by = 1))+\n  scale_y_continuous(limits = c(4,16), breaks = seq(4,16, by = 1))+\n  #facet_wrap(~as.factor(C.YEAR), nrow = 4)+\n  theme_bw()+\n  theme(legend.position='right', \n        legend.title = element_blank(),\n        panel.grid = element_blank(),\n        axis.title = element_text(size = rel(2)),\n        axis.text = element_text(size = rel(1)),\n        strip.text = element_text(size = rel(1.5)),\n        )+\n  labs(x = \"Plant density (pl/m2)\", y = \"Corn yield (Mg/ha)\")\n\nm1_plot\n\n\n\n\n\n\n\n\n\nDocumentation: https://mc-stan.org/rstan/\nBug reports: https://github.com/stan-dev/rstan/issues/\nstan is a stand-alone open-source software platform designed for statistical modeling using high-performance statistical computation applying its own language. When selecting the Bayesian computational approach (i.e.¬†rejection sampling criteria) there are several alternatives to choose. Stan produces Bayesian statistical inference following Hamiltonian Monte Carlo (HMC), and No-U-Turn Samples (NUTS). Besides R, stan has interfaces with other popular languages such as Python, MATLAB, Julia.\nIn contrast to brms, stan‚Äôs syntax is more complicated for begginers, but the positive side is that requires us to write the statistical model.\nWe will not fit a model directly with stan today, but brms brings a function that allows users to obtain the code to run the analysis by ourselves using rstan. Let‚Äôs see‚Ä¶\n\n\n\n\nDocumentation: https://mcmc-jags.sourceforge.io/\nBug reports: https://sourceforge.net/projects/mcmc-jags/\nrjags is another popular option for Bayesian statistical inference following MCMC using R. Rjags produces Bayesian statistical inference following BUGS language (WinBUGS). Similar to stan, rjags it is probably not for beginner, since it requires us to write out the statistical model (although it is always ideal). To extract the posteriors, it also requires coda, which is especially designed for summarizing and plotting MCMC simulations."
  },
  {
    "objectID": "code_4_bayes_3.html",
    "href": "code_4_bayes_3.html",
    "title": "Week #4",
    "section": "",
    "text": "This article is a review and final example to wrap up our brief introduction to Bayesian data analysis.\n\n\n\n\n\n\nImportant\n\n\n\nUnder the Bayesian approach we consider probability as an expression of the degree of certainty (or uncertainty) about a process or parameter.\n\n\n\n\n\n\n\n\nNote\n\n\n\nREMEMBER!\nThe structure of the Bayesian theory is similar to the Human Logic process. It is all about updating knowledge:\n(i) we have some data,\n(ii) we have beliefs about the underlying process,\n(iii) combining both, we can update our beliefs.\n\n\nThus, the Bayes Theorem says\n\\[ P(\\theta | x)  = P(\\theta) * \\frac{P(x|\\theta)}{P(x)}\\] where \\(\\theta\\) is the parameter of interest, \\(x\\) is the data, and ‚Äú|‚Äù means ‚Äúconditional‚Äù.\n\\[ Knowledge~after  = Knowledge~before * updating~factor\\] \\[ Posterior~distribution  = Prior~distribution * \\frac{Likelihood} {Marginal Likelihood} \\]\n\n\nYou may have heard multiple times about Bayesian Hierarchical Framework or Bayesian Hierarchical Modelling. This simply means that our model is compound by multiple levels differing in hierarchy. There are two relevant concepts here to derive the posterior distributions:\n\nHyper-parameters are the parameters of the prior distribution. For example, if we have $ Y|N(, ^2)$, with \\(\\mu = \\beta_0 *X\\) as the ‚Äútop-level‚Äù parameter describing the process model, where \\(\\beta_0\\) is the hyper-parameter.\nHyper-priors are the distributions of the hyper-parameters. For example, \\(\\beta_0 \\sim N(\\beta_0, \\sigma^2_{\\beta_0})\\) is the hyper-prior of \\(\\beta_0\\)\n\nBasically, we have a set of layers:\nLayer 1:\n\\[ y_i|\\mu_i, \\beta_0 \\sim P(y_i|\\mu_i, \\beta_0)\\]\nLayer 2:\n\\[ \\mu_i| \\beta_0 \\sim P(\\mu_i|\\beta_0)\\]\nLayer 3:\n\\[ \\beta_0 \\sim P(\\beta_0)\\]\n\n\n\n\\(P(\\theta|x)\\) is a probability density function that quantifies the ‚Äúuncertainty‚Äù about \\(\\theta\\) within a specific model after the data collection.\nIn practice, however, we never know this specific function. So what we do is to create multiple simulations given the prior/s and the data, and then summarizing those simulations (e.g.¬†obtaining credible intervals at variable probability levels, for example, 95%)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statasaurus‚Äô Homepage",
    "section": "",
    "text": "History\nStatasaurus (a.k.a. Stats-Hub) is a series of encounters designed to share, discuss, learn, and use applied stats and computer programming for agricultural research. It was born in late Fall 2018 from the idea of several lab mates (Adrian Correndo, Santiago Tamagno, Javier Fernandez, and Luiz Moro Rosso) with the support of their advisor (Dr.¬†Ciampitti) at the KSU Crops team (now CiampittiLab).\nStatasaurus was conceived with the main goal sharing knowledge about data analysis using R software. By that time, we were mostly using excel for data wrangling, GraphPad Prism for plots, barely used R script files, no Rmarkdown for reports, and no version control. We didn‚Äôt even know how to create an *.Rproj file. Rapidly, the entire team adopted the idea and the rest is known history. Over the years, many graduate students, visiting scholars, and postdocs have been sharing their coding and stats knowledge (and doubts) allowing the team to exponentially grow in terms of data analytics skills. Nowadays, we are proudly using these meetings exclusively for advanced programming topics. Welcome to the team!\nGeneral Rules\n\n\n\n\n\n\nImportant\n\n\n\nThe #1 and most important rule is: exchange! When you ask for a topic, you also try to offer another topic in exchange.\nThe willing to contribute is of central importance cause we all have duties to take care about. Therefore, self-volunteers are crucial and deeply appreciated.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe meetings are not lectures. Nobody‚Äôs bringing the truth! We are all learning, so we foster the discussion among members.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is expected that members have:\ni. the latest version of RStudio installed.\nii. an active GitHub account, and‚Ä¶\niii. training on the basics of data analysis using R."
  },
  {
    "objectID": "members.html",
    "href": "members.html",
    "title": "Members",
    "section": "",
    "text": "Member\nGitHub user\n\n\n\n\nAdrian Correndo\nhttps://github.com/adriancorrendo\n\n\nIgnacio Massigoge\nhttps://github.com/imassigoge\n\n\nLuciana Nieto\nhttps://github.com/luciananieto\n\n\nValentina Pereyra\nhttps://github.com/valentinapereyra\n\n\nMario Secchi\nhttps://github.com/mariosecchi\n\n\nJosefina Lacasa\nhttps://github.com/jlacasa\n\n\nLuiz Felipe\nhttps://github.com/luizfelipeaa\n\n\nJuan Rybecky\nhttps://github.com/juanrybecky\n\n\nEmma van Versendaal\nhttps://github.com/emmavanver\n\n\nFrancisco Palmero\nhttps://github.com/fpalmeroo\n\n\nDanilo Tedesco\nhttps://github.com/tedescodanilo\n\n\nNilson Vieira\nhttps://github.com/nvieirajr\n\n\nLucia Marziote\nhttps://github.com/Luciamar\n\n\nCarlos Hernandez\nhttps://github.com/Carlitosh\n\n\nAna Carcedo\nhttps://github.com/anajpcarcedo\n\n\nNicolas Giordano\nhttps://github.com/giordanon\n\n\nLara Zini\nhttps://github.com/larazini\n\n\nGustavo Santiago\nhttps://github.com/GustavoSantiago1103\n\n\nIgnacio Rodriguez\nhttps://github.com/nachorodr\n\n\nPia Rodriguez\nhttps://github.com/piarod1r\n\n\nThomas Varela\nhttps://github.com/thomasvarela\n\n\nLeonardo Bosche\n-\n\n\nAgustina Go√±i\n-\n\n\nYesica Chazarreta\n-"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "This section is intended to serve as a repo where we can share docs, papers, links, tutorials, etc. useful for each topic as well as random content."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Fall 2022 - Weekly meeting schedule\n\n\nüóìÔ∏èDate | |\nü•ïTopic | |\nüôãüèªMembers\n| | GitHub \n\n\n\n\n29-08-2022\nIntro, GitHub, Quarto\nAdrian\nhttps://github.com/adriancorrendo/Stats_Hub_2022\n\n\n05-09-2022\nLabor Day\n-\n\n\n\n12-09-2022\nHierarchical Bayesian Models I - Theory & Discussion\nAdrian and Josefina\nhttps://adriancorrendo.github.io/statasaurusweb/code_2_bayes_1.html\n\n\n19-09-2022\nHierarchical Bayesian Models II - Applied Examples with rjags, rstan , brms \nAdrian and Josefina\nhttps://adriancorrendo.github.io/statasaurusweb/code_3_bayes_2.html\n\n\n26-09-2022\nHierarchical Bayesian Models III\nAdrian and Josefina\n\n\n\n03-10-2022\nShinyapps \nAdrian\n\n\n\n10-10-2022\nRun APSIM in R, apsimx\nAna\n\n\n\n17-10-2022\nMachine Learning, tidymodels, H2O \nAdrian\n\n\n\n24-10-2022\nSystematic Reviews,\nMeta-analysis protocols\nNicolas\n\n\n\n31-10-2022\nAdvanced ggplot ,\nData digitization packages (juicr)\nAdrian, Emma, Juan\n\n\n\n07-11-2022\nASA Meetings‚Äô Week\n-\n\n\n\n14-11-2022\nPrediction Error Metrics, metrica \nAdrian, Luciana\n\n\n\n21-11-2022\nThanksgiving\n-\n\n\n\n28-11-2022\nCreating a package, devtools , roxygen2\nAdrian\n\n\n\n05-12-2022\nConclusions & next steps\n-"
  }
]